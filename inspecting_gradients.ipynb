{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3884b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from functions import load_simple_scenarios_with_flexible_context\n",
    "from pald_implementation import (\n",
    "    make_pald_base_layer,\n",
    "    make_pald_flex_purchase_layer,\n",
    "    make_pald_flex_delivery_layer,\n",
    "    compute_segment_caps\n",
    ")\n",
    "from paad_implementation import get_alpha\n",
    "from robust_projection import project_y_robust, project_y_flex_robust\n",
    "from contextual_model import ThresholdPredictor\n",
    "import paad_implementation as pi\n",
    "import math\n",
    "from paad_implementation import objective_function as np_objective_function\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import opt_sol\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6b4dd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10           # number of segments in piecewise linear approximation for psi\n",
    "gamma = 1.0     # switching cost parameter for x\n",
    "delta = 0.5     # switching cost parameter for z (used in analytical threshold)\n",
    "S = 1.0          # maximum inventory capacity\n",
    "c_delivery = 0.2\n",
    "eps_delivery = 0.05\n",
    "epochs = 10\n",
    "T = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f792cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a toy instance of osdm where prices decrease from 10 to 1 and then jump up to 10, \n",
    "# single unit of base demand at T=10, no flex demand\n",
    "p_min = 1.0\n",
    "p_max = 10.0\n",
    "base_demand_all = [1.0 if t == T-1 else 0.0 for t in range(T)]\n",
    "flex_demand_all = [0.0 for t in range(T)]\n",
    "\n",
    "prices = np.linspace(p_max, p_min, T-1).tolist()\n",
    "prices.append(p_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eacf501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_options = {\n",
    "    # SCS parameters tend to be robust for differentiable layers\n",
    "    \"eps\": 1e-5,\n",
    "    \"max_iters\": 2000,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def _safe_layer_call(layer, args, size=1.0):\n",
    "    \"\"\"\n",
    "    Call a CvxpyLayer and catch SCS/diffcp failures. Returns x_total tensor.\n",
    "    \"\"\"\n",
    "    x_total, x_parts = layer(*args, solver_args=solver_options)\n",
    "    return x_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "012e4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pald_base_layer(K, gamma, ridge=False):\n",
    "    x_parts = cp.Variable(K, nonneg=True)\n",
    "    x_total = cp.Variable(nonneg=True)\n",
    "\n",
    "    x_prev = cp.Parameter(nonneg=True)\n",
    "    w_prev = cp.Parameter(nonneg=True)\n",
    "    p_t = cp.Parameter()\n",
    "    y_vec = cp.Parameter((K,))\n",
    "    caps = cp.Parameter((K,), nonneg=True)\n",
    "\n",
    "    constraints = [\n",
    "        x_parts >= 0,\n",
    "        x_parts <= caps,\n",
    "        x_total == cp.sum(x_parts),\n",
    "        x_total <= 1 - w_prev,\n",
    "    ]\n",
    "    ridge = 0\n",
    "    if ridge:\n",
    "        # Increase ridge to encourage interior solutions and smoother differentiability\n",
    "        ridge = 1e-3 * cp.sum_squares(x_parts) + 1e-3 * cp.sum_squares(x_total)\n",
    "    hit_cost = p_t * x_total\n",
    "    switch_cost = gamma * cp.abs(x_total - x_prev) + gamma * cp.abs(x_total)\n",
    "    phi_cost = y_vec @ x_parts\n",
    "    obj = cp.Minimize(hit_cost + switch_cost - phi_cost + ridge)\n",
    "    prob = cp.Problem(obj, constraints)\n",
    "    return CvxpyLayer(prob,\n",
    "                      parameters=[x_prev, w_prev, p_t, y_vec, caps],\n",
    "                      variables=[x_total, x_parts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "45ac56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to compute a (coarse approximation) of the integral over the (piecewise-affine) threshold function phi\n",
    "def compute_segment_caps(w_prev: float, K: int):\n",
    "    \"\"\"Remaining capacity per segment given cumulative fraction w_prev.\"\"\"\n",
    "    # Clamp w into [0, 1]\n",
    "    w = max(0.0, min(1.0, float(w_prev)))\n",
    "    if 1.0 - w <= 1e-9:\n",
    "        return [0.0] * K\n",
    "    caps = []\n",
    "    for i in range(K):\n",
    "        left = i / K\n",
    "        right = (i + 1) / K\n",
    "        cap = max(0.0, right - max(left, w))\n",
    "        caps.append(cap)\n",
    "    return caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5ad9afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiable torch objective function\n",
    "# NOTE: Keep everything as torch ops; avoid Python floats that can break the graph.\n",
    "def torch_objective(p_seq, x_seq, z_seq, gamma, delta, c, eps):\n",
    "    \"\"\"Torch version of objective_function for differentiable PALD cost.\n",
    "    Inputs are torch 1D tensors of length T (float32).\n",
    "    Mirrors paad_implementation.objective_function.\n",
    "    \"\"\"\n",
    "    Tn = p_seq.shape[0]\n",
    "    # state of charge s[0..T]\n",
    "    s = []\n",
    "    s_prev = torch.zeros((), dtype=torch.float32)\n",
    "    s.append(s_prev)\n",
    "    for t in range(1, Tn + 1):\n",
    "        s_t = torch.clamp(s_prev + x_seq[t - 1] - z_seq[t - 1], min=0.0)\n",
    "        s.append(s_t)\n",
    "        s_prev = s_t\n",
    "    s_torch = torch.stack(s)\n",
    "\n",
    "    # Costs\n",
    "    cost_purchasing = (p_seq * x_seq).sum()\n",
    "    switching_cost_x = gamma * (x_seq[1:] - x_seq[:-1]).abs().sum() if Tn > 1 else torch.tensor(0.0)\n",
    "    switching_cost_z = delta * (z_seq[1:] - z_seq[:-1]).abs().sum() if Tn > 1 else torch.tensor(0.0)\n",
    "    s_prev_seq = s_torch[:-1]\n",
    "    discharge_cost = (p_seq * (c * z_seq + eps * z_seq - c * s_prev_seq * z_seq)).sum()\n",
    "    return cost_purchasing + switching_cost_x + switching_cost_z + discharge_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3f6a7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base demand cvxpylayer\n",
    "pald_base_layer = make_pald_base_layer(K, gamma, ridge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fc613cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [0.0, 0.0, 1.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "Cost of this solution: 10.75\n",
      "Gradients of cost w.r.t. y_vec (large init): tensor([6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09,\n",
      "        6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09])\n",
      "New y_vec after one gradient step: tensor([9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936,\n",
      "        9.9936], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# y_vec is the values of the piecewise affine threshold function -- let's see what happens if we set it HIGH\n",
    "# IMPORTANT: To get valid gradients w.r.t. y_vec, we must keep decisions as torch Tensors and avoid .item()/.round()\n",
    "y_vec_large = torch.tensor([p_max for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "def simulate_one_driver(y_vec, prices, base_demand_all, flex_demand_all, ridge=False):\n",
    "    \"\"\"\n",
    "    Simulate one base driver with given y_vec and return a differentiable loss.\n",
    "    Decisions remain as torch.Tensors so autograd can backprop to y_vec through the CvxpyLayer.\n",
    "    \"\"\"\n",
    "    # torch state (kept differentiable across time)\n",
    "    x_prev = torch.tensor(0.0, dtype=torch.float32)  # previous decision (scalar)\n",
    "    w_prev = torch.tensor(0.0, dtype=torch.float32)  # cumulative fraction delivered\n",
    "\n",
    "    torch_decisions = []\n",
    "\n",
    "    for t in range(T):\n",
    "        p_t = prices[t]\n",
    "        # Prepare parameters for the layer\n",
    "        x_prev_t = x_prev.view(1)\n",
    "        w_prev_t = w_prev.view(1)\n",
    "        p_t_t = torch.tensor([p_t], dtype=torch.float32)\n",
    "\n",
    "        # Segment caps are not differentiated; treat them as constants derived from current w_prev\n",
    "        caps_list = compute_segment_caps(float(w_prev.detach().cpu().item()), K)\n",
    "        caps_t = torch.tensor(caps_list, dtype=torch.float32)\n",
    "\n",
    "        # Solve the per-step convex problem via CVXPYLayer (returns x_total, x_parts)\n",
    "        x_total_t = _safe_layer_call(\n",
    "            pald_base_layer, (x_prev_t, w_prev_t, p_t_t, y_vec, caps_t)\n",
    "        )\n",
    "        # Squeeze to scalar for arithmetic\n",
    "        x_total_scalar = x_total_t.squeeze()\n",
    "\n",
    "        # On the final step, force completion if any remaining fraction < 1.0\n",
    "        if t == T - 1:\n",
    "            # Add just enough slack to finish any remainder (keeps graph intact)\n",
    "            remainder = torch.clamp(1.0 - (w_prev + x_total_scalar), min=0.0)\n",
    "            x_total_scalar = x_total_scalar + remainder\n",
    "\n",
    "        torch_decisions.append(x_total_scalar)\n",
    "\n",
    "        # Update state for the next step (kept differentiable)\n",
    "        w_prev = w_prev + x_total_scalar\n",
    "        x_prev = x_total_scalar\n",
    "\n",
    "    # Stack decisions and compute differentiable objective\n",
    "    x_seq = torch.stack(torch_decisions)  # shape [T]\n",
    "    z_seq = torch.tensor(base_demand_all, dtype=torch.float32)  # no flex here; constant\n",
    "\n",
    "    loss = torch_objective(\n",
    "        torch.tensor(prices, dtype=torch.float32),\n",
    "        x_seq,\n",
    "        z_seq,\n",
    "        gamma,\n",
    "        delta,\n",
    "        c_delivery,\n",
    "        eps_delivery,\n",
    "    )\n",
    "\n",
    "    # For visibility only (no graph break for the loss)\n",
    "    with torch.no_grad():\n",
    "        pretty = [round(float(v), 4) for v in x_seq]\n",
    "        print(\"Decisions:\", pretty)\n",
    "        print(\"Cost of this solution:\", float(loss))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Run and backprop to inspect gradients w.r.t y_vec\n",
    "loss = simulate_one_driver(y_vec_large, prices, base_demand_all, flex_demand_all)\n",
    "loss.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (large init):\", y_vec_large.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_large - lr * y_vec_large.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "636038f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [-0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 1.0]\n",
      "Cost of this solution: 14.0\n",
      "Gradients of cost w.r.t. y_vec (small init): tensor([3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10,\n",
      "        3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10])\n",
      "New y_vec after one gradient step: tensor([0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996,\n",
      "        0.9996], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# what if we make y_vec small?\n",
    "y_vec_small = torch.tensor([p_min for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "loss_small = simulate_one_driver(y_vec_small, prices, base_demand_all, flex_demand_all)\n",
    "loss_small.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (small init):\", y_vec_small.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_small - lr * y_vec_small.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3b0a6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above is a big problem because the threshold is now outside the range [p_min, p_max] !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7d8f003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal decisions:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0]\n",
      "Optimal cost:  3.5625\n"
     ]
    }
   ],
   "source": [
    "# what is the optimal solution?\n",
    "flex_demand_all = [0.0 for t in range(T)]\n",
    "Deltas = [0.0 for t in range(T)]\n",
    "status, results = opt_sol.optimal_solution(T, prices, gamma, delta, c_delivery, eps_delivery, S, base_demand_all, flex_demand_all, Deltas)\n",
    "if status == \"Optimal\" and results is not None:\n",
    "    decisions = results['x']\n",
    "    opt_z = results['z']\n",
    "    opt_s = results['s'][1:]\n",
    "    # Use numpy objective for consistency\n",
    "    opt_cost = np_objective_function(T, prices, gamma, delta, c_delivery, eps_delivery, decisions, opt_z)\n",
    "decisions = [round(d, 4) for d in decisions]\n",
    "print(\"Optimal decisions: \", decisions)\n",
    "print(\"Optimal cost: \", np_objective_function(T, prices, gamma, delta, c_delivery, eps_delivery, decisions, base_demand_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5711ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we add a penalty to encourage the drivers to \"purchase early\" (instead of relying on compulsory trade)\n",
    "\n",
    "def simulate_one_driver_penalty(y_vec, prices, base_demand_all, flex_demand_all):\n",
    "    \"\"\"\n",
    "    Simulate one base driver with given y_vec and return a differentiable loss.\n",
    "    Decisions remain as torch.Tensors so autograd can backprop to y_vec through the CvxpyLayer.\n",
    "    \"\"\"\n",
    "    # torch state (kept differentiable across time)\n",
    "    x_prev = torch.tensor(0.0, dtype=torch.float32)  # previous decision (scalar)\n",
    "    w_prev = torch.tensor(0.0, dtype=torch.float32)  # cumulative fraction delivered\n",
    "    x_topup = torch.tensor(0.0, dtype=torch.float32)\n",
    "\n",
    "    torch_decisions = []\n",
    "\n",
    "    for t in range(T):\n",
    "        p_t = prices[t]\n",
    "        # Prepare parameters for the layer\n",
    "        x_prev_t = x_prev.view(1)\n",
    "        w_prev_t = w_prev.view(1)\n",
    "        p_t_t = torch.tensor([p_t], dtype=torch.float32)\n",
    "\n",
    "        # Segment caps are not differentiated; treat them as constants derived from current w_prev\n",
    "        caps_list = compute_segment_caps(float(w_prev.detach().cpu().item()), K)\n",
    "        caps_t = torch.tensor(caps_list, dtype=torch.float32)\n",
    "\n",
    "        # Solve the per-step convex problem via CVXPYLayer (returns x_total, x_parts)\n",
    "        x_total_t = _safe_layer_call(\n",
    "            pald_base_layer, (x_prev_t, w_prev_t, p_t_t, y_vec, caps_t)\n",
    "        )\n",
    "        # Squeeze to scalar for arithmetic\n",
    "        x_total_scalar = x_total_t.squeeze()\n",
    "\n",
    "        # On the final step, force completion if any remaining fraction < 1.0\n",
    "        if t == T - 1:\n",
    "            # Add just enough slack to finish any remainder (keeps graph intact)\n",
    "            remainder = torch.clamp(1.0 - (w_prev + x_total_scalar), min=0.0)\n",
    "            x_total_scalar = x_total_scalar + remainder\n",
    "            x_topup = remainder\n",
    "\n",
    "        torch_decisions.append(x_total_scalar)\n",
    "\n",
    "        # Update state for the next step (kept differentiable)\n",
    "        w_prev = w_prev + x_total_scalar\n",
    "        x_prev = x_total_scalar\n",
    "\n",
    "    # Stack decisions and compute differentiable objective\n",
    "    x_seq = torch.stack(torch_decisions)  # shape [T]\n",
    "    z_seq = torch.tensor(base_demand_all, dtype=torch.float32)\n",
    "\n",
    "    loss = torch_objective(\n",
    "        torch.tensor(prices, dtype=torch.float32),\n",
    "        x_seq,\n",
    "        z_seq,\n",
    "        gamma,\n",
    "        delta,\n",
    "        c_delivery,\n",
    "        eps_delivery,\n",
    "    )\n",
    "    # add a penalty to the loss proportional to x_topup\n",
    "    lamda = torch.tensor(200.0, dtype=torch.float32)\n",
    "    loss += lamda * remainder\n",
    "\n",
    "    # For visibility only (no graph break for the loss)\n",
    "    with torch.no_grad():\n",
    "        pretty = [round(float(v), 4) for v in x_seq]\n",
    "        print(\"Decisions:\", pretty)\n",
    "        print(\"Augmented cost of this solution:\", float(loss))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "93a550e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [0.0, 0.0, 1.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "Augmented cost of this solution: 10.75\n",
      "Gradients of augmented cost w.r.t. y_vec (large init): tensor([6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09,\n",
      "        6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09])\n",
      "New y_vec after one gradient step: tensor([9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936,\n",
      "        9.9936], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_vec_large = torch.tensor([p_max for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "# Run and backprop to inspect gradients w.r.t y_vec\n",
    "loss = simulate_one_driver_penalty(y_vec_large, prices, base_demand_all, flex_demand_all)\n",
    "loss.backward()\n",
    "print(\"Gradients of augmented cost w.r.t. y_vec (large init):\", y_vec_large.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_large - lr * y_vec_large.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6663a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [-0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 1.0]\n",
      "Augmented cost of this solution: 214.0\n",
      "Gradients of cost w.r.t. y_vec (small init): tensor([6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08,\n",
      "        6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08])\n",
      "New y_vec after one gradient step: tensor([0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352,\n",
      "        0.9352], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# what if we make y_vec small?\n",
    "y_vec_small = torch.tensor([p_min for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "loss_small = simulate_one_driver_penalty(y_vec_small, prices, base_demand_all, flex_demand_all)\n",
    "loss_small.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (small init):\", y_vec_small.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_small - lr * y_vec_small.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1e300279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even if I add a large penalty to the compulsory trade (forced top-ups), the gradient is still pointing down?\n",
    "\n",
    "# when I train using train_pald_contextual.py, this is one of the key issues I run into -- the NN learns that it should predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d40cb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retry with ridge enabled, but no penalty\n",
    "# define the base demand cvxpylayer\n",
    "pald_base_layer = make_pald_base_layer(K, gamma, ridge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f3a38b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [0.0, 0.0, 1.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "Cost of this solution: 10.75\n",
      "Gradients of cost w.r.t. y_vec (large init): tensor([6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09,\n",
      "        6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09])\n",
      "New y_vec after one gradient step: tensor([9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936,\n",
      "        9.9936], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_vec_large = torch.tensor([p_max for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "# Run and backprop to inspect gradients w.r.t y_vec\n",
    "loss = simulate_one_driver(y_vec_large, prices, base_demand_all, flex_demand_all)\n",
    "loss.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (large init):\", y_vec_large.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_large - lr * y_vec_large.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "aa03020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [-0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 1.0]\n",
      "Cost of this solution: 14.0\n",
      "Gradients of cost w.r.t. y_vec (small init): tensor([3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10,\n",
      "        3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10])\n",
      "New y_vec after one gradient step: tensor([0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996,\n",
      "        0.9996], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# what if we make y_vec small?\n",
    "y_vec_small = torch.tensor([p_min for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "loss_small = simulate_one_driver(y_vec_small, prices, base_demand_all, flex_demand_all)\n",
    "loss_small.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (small init):\", y_vec_small.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_small - lr * y_vec_small.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9781b383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [0.0, 0.0, 1.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "Augmented cost of this solution: 10.75\n",
      "Gradients of augmentedcost w.r.t. y_vec (large init): tensor([6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09,\n",
      "        6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09])\n",
      "New y_vec after one gradient step: tensor([9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936,\n",
      "        9.9936], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/miniconda3/envs/pald/lib/python3.10/site-packages/scs/__init__.py:83: UserWarning: Converting A to a CSC (compressed sparse column) matrix; may take a while.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# with penalty and ridge\n",
    "pald_base_layer = make_pald_base_layer(K, gamma, ridge=True)\n",
    "y_vec_large = torch.tensor([p_max for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "# Run and backprop to inspect gradients w.r.t y_vec\n",
    "loss = simulate_one_driver_penalty(y_vec_large, prices, base_demand_all, flex_demand_all)\n",
    "loss.backward()\n",
    "print(\"Gradients of augmentedcost w.r.t. y_vec (large init):\", y_vec_large.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_large - lr * y_vec_large.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "568df746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [-0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 1.0]\n",
      "Augmented cost of this solution: 214.0\n",
      "Gradients of cost w.r.t. y_vec (small init): tensor([6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08,\n",
      "        6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08])\n",
      "New y_vec after one gradient step: tensor([0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352,\n",
      "        0.9352], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/miniconda3/envs/pald/lib/python3.10/site-packages/scs/__init__.py:83: UserWarning: Converting A to a CSC (compressed sparse column) matrix; may take a while.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# what if we make y_vec small?\n",
    "y_vec_small = torch.tensor([p_min for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "loss_small = simulate_one_driver_penalty(y_vec_small, prices, base_demand_all, flex_demand_all)\n",
    "loss_small.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (small init):\", y_vec_small.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_small - lr * y_vec_small.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867330d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pald",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
