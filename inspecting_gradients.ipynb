{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "3884b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from functions import load_simple_scenarios_with_flexible_context\n",
    "from pald_implementation import (\n",
    "    make_pald_base_layer,\n",
    "    make_pald_flex_purchase_layer,\n",
    "    make_pald_flex_delivery_layer,\n",
    "    compute_segment_caps\n",
    ")\n",
    "from paad_implementation import get_alpha\n",
    "from robust_projection import project_y_robust, project_y_flex_robust\n",
    "from contextual_model import ThresholdPredictor\n",
    "import paad_implementation as pi\n",
    "import math\n",
    "from paad_implementation import objective_function as np_objective_function\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import opt_sol\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "6b4dd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10           # number of segments in piecewise linear approximation for psi\n",
    "gamma = 1.0     # switching cost parameter for x\n",
    "delta = 0.5     # switching cost parameter for z (used in analytical threshold)\n",
    "S = 1.0          # maximum inventory capacity\n",
    "c_delivery = 0.2\n",
    "eps_delivery = 0.05\n",
    "epochs = 10\n",
    "T = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "f792cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a toy instance of osdm where prices decrease from 10 to 1 and then jump up to 10, \n",
    "# single unit of base demand at T=10, no flex demand\n",
    "p_min = 1.0\n",
    "p_max = 10.0\n",
    "base_demand_all = [1.0 if t == T-1 else 0.0 for t in range(T)]\n",
    "flex_demand_all = [0.0 for t in range(T)]\n",
    "\n",
    "prices = np.linspace(p_max, p_min, T-1).tolist()\n",
    "prices.append(p_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "eacf501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_options = {\n",
    "    # SCS parameters tend to be robust for differentiable layers\n",
    "    \"eps\": 1e-5,\n",
    "    \"max_iters\": 2000,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def _safe_layer_call(layer, args, size=1.0):\n",
    "    \"\"\"\n",
    "    Call a CvxpyLayer and catch SCS/diffcp failures. Returns x_total tensor.\n",
    "    \"\"\"\n",
    "    x_total, x_parts = layer(*args, solver_args=solver_options)\n",
    "    return x_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "012e4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pald_base_layer(K, gamma, ridge=False):\n",
    "    x_parts = cp.Variable(K, nonneg=True)\n",
    "    x_total = cp.Variable(nonneg=True)\n",
    "\n",
    "    x_prev = cp.Parameter(nonneg=True)\n",
    "    w_prev = cp.Parameter(nonneg=True)\n",
    "    p_t = cp.Parameter()\n",
    "    y_vec = cp.Parameter((K,))\n",
    "    caps = cp.Parameter((K,), nonneg=True)\n",
    "\n",
    "    constraints = [\n",
    "        x_parts >= 0,\n",
    "        x_parts <= caps,\n",
    "        x_total == cp.sum(x_parts),\n",
    "        x_total <= 1 - w_prev,\n",
    "    ]\n",
    "    ridge = 0\n",
    "    if ridge:\n",
    "        # Increase ridge to encourage interior solutions and smoother differentiability\n",
    "        ridge = 1e-3 * cp.sum_squares(x_parts) + 1e-3 * cp.sum_squares(x_total)\n",
    "    hit_cost = p_t * x_total\n",
    "    switch_cost = gamma * cp.abs(x_total - x_prev) + gamma * cp.abs(x_total)\n",
    "    phi_cost = y_vec @ x_parts\n",
    "    # write down the closed-form of the messy piecewise quadratic integral - increasing and concave \n",
    "    # decomposing into x_parts -- assumptions -- double check (discontinuities) -- define as min or max of quadratic functions -- don't need to do this decomposition into x_part\n",
    "    \n",
    "    # each x_part corresponds to a quadratic function\n",
    "    obj = cp.Minimize(hit_cost + switch_cost - phi_cost + ridge)\n",
    "    prob = cp.Problem(obj, constraints)\n",
    "    return CvxpyLayer(prob,\n",
    "                      parameters=[x_prev, w_prev, p_t, y_vec, caps],\n",
    "                      variables=[x_total, x_parts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "45ac56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to compute a (coarse approximation) of the integral over the (piecewise-affine) threshold function phi\n",
    "def compute_segment_caps(w_prev: float, K: int):\n",
    "    \"\"\"Remaining capacity per segment given cumulative fraction w_prev.\"\"\"\n",
    "    # Clamp w into [0, 1]\n",
    "    w = max(0.0, min(1.0, float(w_prev)))\n",
    "    if 1.0 - w <= 1e-9:\n",
    "        return [0.0] * K\n",
    "    caps = []\n",
    "    for i in range(K):\n",
    "        left = i / K\n",
    "        right = (i + 1) / K\n",
    "        cap = max(0.0, right - max(left, w))\n",
    "        caps.append(cap)\n",
    "    return caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "5ad9afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiable torch objective function\n",
    "# NOTE: Keep everything as torch ops; avoid Python floats that can break the graph.\n",
    "def torch_objective(p_seq, x_seq, z_seq, gamma, delta, c, eps):\n",
    "    \"\"\"Torch version of objective_function for differentiable PALD cost.\n",
    "    Inputs are torch 1D tensors of length T (float32).\n",
    "    Mirrors paad_implementation.objective_function.\n",
    "    \"\"\"\n",
    "    Tn = p_seq.shape[0]\n",
    "    # state of charge s[0..T]\n",
    "    s = []\n",
    "    s_prev = torch.zeros(1, dtype=p_seq.dtype, device=p_seq.device)\n",
    "    s.append(s_prev)\n",
    "    for t in range(1, Tn + 1):\n",
    "        s_t = torch.clamp(s_prev + x_seq[t - 1] - z_seq[t - 1], min=0.0)\n",
    "        s.append(s_t)\n",
    "        s_prev = s_t\n",
    "    s_torch = torch.cat(s, dim=0)\n",
    "\n",
    "    # Costs\n",
    "    cost_purchasing = (p_seq * x_seq).sum()\n",
    "    switching_cost_x = gamma * (x_seq[1:] - x_seq[:-1]).abs().sum() if Tn > 1 else torch.tensor(0.0)\n",
    "    switching_cost_z = delta * (z_seq[1:] - z_seq[:-1]).abs().sum() if Tn > 1 else torch.tensor(0.0)\n",
    "    s_prev_seq = s_torch[:-1]\n",
    "    discharge_cost = (p_seq * (c * z_seq + eps * z_seq - c * s_prev_seq * z_seq)).sum()\n",
    "    return cost_purchasing + switching_cost_x + switching_cost_z + discharge_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "3f6a7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base demand cvxpylayer\n",
    "pald_base_layer = make_pald_base_layer(K, gamma, ridge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "fc613cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [0.0, 0.0, 1.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "Cost of this solution: 10.75\n",
      "Gradients of cost w.r.t. y_vec (large init): tensor([6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09,\n",
      "        6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09])\n",
      "New y_vec after one gradient step: tensor([9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936,\n",
      "        9.9936], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# y_vec is the values of the piecewise affine threshold function -- let's see what happens if we set it HIGH\n",
    "# IMPORTANT: To get valid gradients w.r.t. y_vec, we must keep decisions as torch Tensors and avoid .item()/.round()\n",
    "y_vec_large = torch.tensor([p_max for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "def simulate_one_driver(y_vec, prices, base_demand_all, flex_demand_all, ridge=False):\n",
    "    \"\"\"\n",
    "    Simulate one base driver with given y_vec and return a differentiable loss.\n",
    "    Decisions remain as torch.Tensors so autograd can backprop to y_vec through the CvxpyLayer.\n",
    "    \"\"\"\n",
    "    # torch state (kept differentiable across time)\n",
    "    x_prev = torch.tensor(0.0, dtype=torch.float32)  # previous decision (scalar)\n",
    "    w_prev = torch.tensor(0.0, dtype=torch.float32)  # cumulative fraction delivered\n",
    "\n",
    "    torch_decisions = []\n",
    "\n",
    "    for t in range(T):\n",
    "        p_t = prices[t]\n",
    "        # Prepare parameters for the layer\n",
    "        x_prev_t = x_prev.view(1)\n",
    "        w_prev_t = w_prev.view(1)\n",
    "        p_t_t = torch.tensor([p_t], dtype=torch.float32)\n",
    "\n",
    "        # Segment caps are not differentiated; treat them as constants derived from current w_prev\n",
    "        caps_list = compute_segment_caps(float(w_prev.detach().cpu().item()), K)\n",
    "        caps_t = torch.tensor(caps_list, dtype=torch.float32)\n",
    "\n",
    "        # Solve the per-step convex problem via CVXPYLayer (returns x_total, x_parts)\n",
    "        x_total_t = _safe_layer_call(\n",
    "            pald_base_layer, (x_prev_t, w_prev_t, p_t_t, y_vec, caps_t)\n",
    "        )\n",
    "        # Squeeze to scalar for arithmetic\n",
    "        x_total_scalar = x_total_t.squeeze()\n",
    "\n",
    "        # On the final step, force completion if any remaining fraction < 1.0\n",
    "        if t == T - 1:\n",
    "            # Add just enough slack to finish any remainder (keeps graph intact)\n",
    "            remainder = torch.clamp(1.0 - (w_prev + x_total_scalar), min=0.0)\n",
    "            x_total_scalar = x_total_scalar + remainder\n",
    "\n",
    "        torch_decisions.append(x_total_scalar)\n",
    "\n",
    "        # Update state for the next step (kept differentiable)\n",
    "        w_prev = w_prev + x_total_scalar\n",
    "        x_prev = x_total_scalar\n",
    "\n",
    "    # Stack decisions and compute differentiable objective\n",
    "    x_seq = torch.stack(torch_decisions)  # shape [T]\n",
    "    z_seq = torch.tensor(base_demand_all, dtype=torch.float32)  # no flex here; constant\n",
    "\n",
    "    loss = torch_objective(\n",
    "        torch.tensor(prices, dtype=torch.float32),\n",
    "        x_seq,\n",
    "        z_seq,\n",
    "        gamma,\n",
    "        delta,\n",
    "        c_delivery,\n",
    "        eps_delivery,\n",
    "    )\n",
    "\n",
    "    # For visibility only (no graph break for the loss)\n",
    "    with torch.no_grad():\n",
    "        pretty = [round(float(v), 4) for v in x_seq]\n",
    "        print(\"Decisions:\", pretty)\n",
    "        print(\"Cost of this solution:\", float(loss))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Run and backprop to inspect gradients w.r.t y_vec\n",
    "loss = simulate_one_driver(y_vec_large, prices, base_demand_all, flex_demand_all)\n",
    "loss.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (large init):\", y_vec_large.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_large - lr * y_vec_large.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "636038f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [-0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 1.0]\n",
      "Cost of this solution: 14.0\n",
      "Gradients of cost w.r.t. y_vec (small init): tensor([3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10,\n",
      "        3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10])\n",
      "New y_vec after one gradient step: tensor([0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996,\n",
      "        0.9996], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# what if we make y_vec small?\n",
    "y_vec_small = torch.tensor([p_min for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "loss_small = simulate_one_driver(y_vec_small, prices, base_demand_all, flex_demand_all)\n",
    "loss_small.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (small init):\", y_vec_small.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_small - lr * y_vec_small.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "3b0a6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above is a big problem because the threshold is now outside the range [p_min, p_max] !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "7d8f003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal decisions:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0]\n",
      "Optimal cost:  3.5625\n"
     ]
    }
   ],
   "source": [
    "# what is the optimal solution?\n",
    "flex_demand_all = [0.0 for t in range(T)]\n",
    "Deltas = [0.0 for t in range(T)]\n",
    "status, results = opt_sol.optimal_solution(T, prices, gamma, delta, c_delivery, eps_delivery, S, base_demand_all, flex_demand_all, Deltas)\n",
    "if status == \"Optimal\" and results is not None:\n",
    "    decisions = results['x']\n",
    "    opt_z = results['z']\n",
    "    opt_s = results['s'][1:]\n",
    "    # Use numpy objective for consistency\n",
    "    opt_cost = np_objective_function(T, prices, gamma, delta, c_delivery, eps_delivery, decisions, opt_z)\n",
    "decisions = [round(d, 4) for d in decisions]\n",
    "print(\"Optimal decisions: \", decisions)\n",
    "print(\"Optimal cost: \", np_objective_function(T, prices, gamma, delta, c_delivery, eps_delivery, decisions, base_demand_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "5711ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we add a penalty to encourage the drivers to \"purchase early\" (instead of relying on compulsory trade)\n",
    "\n",
    "def simulate_one_driver_penalty(y_vec, prices, base_demand_all, flex_demand_all):\n",
    "    \"\"\"\n",
    "    Simulate one base driver with given y_vec and return a differentiable loss.\n",
    "    Decisions remain as torch.Tensors so autograd can backprop to y_vec through the CvxpyLayer.\n",
    "    \"\"\"\n",
    "    # torch state (kept differentiable across time)\n",
    "    x_prev = torch.tensor(0.0, dtype=torch.float32)  # previous decision (scalar)\n",
    "    w_prev = torch.tensor(0.0, dtype=torch.float32)  # cumulative fraction delivered\n",
    "    x_topup = torch.tensor(0.0, dtype=torch.float32)\n",
    "\n",
    "    torch_decisions = []\n",
    "\n",
    "    for t in range(T):\n",
    "        p_t = prices[t]\n",
    "        # Prepare parameters for the layer\n",
    "        x_prev_t = x_prev.view(1)\n",
    "        w_prev_t = w_prev.view(1)\n",
    "        p_t_t = torch.tensor([p_t], dtype=torch.float32)\n",
    "\n",
    "        # Segment caps are not differentiated; treat them as constants derived from current w_prev\n",
    "        caps_list = compute_segment_caps(float(w_prev.detach().cpu().item()), K)\n",
    "        caps_t = torch.tensor(caps_list, dtype=torch.float32)\n",
    "\n",
    "        # Solve the per-step convex problem via CVXPYLayer (returns x_total, x_parts)\n",
    "        x_total_t = _safe_layer_call(\n",
    "            pald_base_layer, (x_prev_t, w_prev_t, p_t_t, y_vec, caps_t)\n",
    "        )\n",
    "        # Squeeze to scalar for arithmetic\n",
    "        x_total_scalar = x_total_t.squeeze()\n",
    "\n",
    "        # On the final step, force completion if any remaining fraction < 1.0\n",
    "        if t == T - 1:\n",
    "            # Add just enough slack to finish any remainder (keeps graph intact)\n",
    "            remainder = torch.clamp(1.0 - (w_prev + x_total_scalar), min=0.0)\n",
    "            x_total_scalar = x_total_scalar + remainder\n",
    "            x_topup = remainder\n",
    "\n",
    "        torch_decisions.append(x_total_scalar)\n",
    "\n",
    "        # Update state for the next step (kept differentiable)\n",
    "        w_prev = w_prev + x_total_scalar\n",
    "        x_prev = x_total_scalar\n",
    "\n",
    "    # Stack decisions and compute differentiable objective\n",
    "    x_seq = torch.stack(torch_decisions)  # shape [T]\n",
    "    z_seq = torch.tensor(base_demand_all, dtype=torch.float32)\n",
    "\n",
    "    loss = torch_objective(\n",
    "        torch.tensor(prices, dtype=torch.float32),\n",
    "        x_seq,\n",
    "        z_seq,\n",
    "        gamma,\n",
    "        delta,\n",
    "        c_delivery,\n",
    "        eps_delivery,\n",
    "    )\n",
    "    # add a penalty to the loss proportional to x_topup\n",
    "    lamda = torch.tensor(200.0, dtype=torch.float32)\n",
    "    loss += lamda * remainder\n",
    "\n",
    "    # For visibility only (no graph break for the loss)\n",
    "    with torch.no_grad():\n",
    "        pretty = [round(float(v), 4) for v in x_seq]\n",
    "        print(\"Decisions:\", pretty)\n",
    "        print(\"Augmented cost of this solution:\", float(loss))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "93a550e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [0.0, 0.0, 1.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "Augmented cost of this solution: 10.75\n",
      "Gradients of augmented cost w.r.t. y_vec (large init): tensor([6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09,\n",
      "        6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09])\n",
      "New y_vec after one gradient step: tensor([9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936,\n",
      "        9.9936], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_vec_large = torch.tensor([p_max for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "# Run and backprop to inspect gradients w.r.t y_vec\n",
    "loss = simulate_one_driver_penalty(y_vec_large, prices, base_demand_all, flex_demand_all)\n",
    "loss.backward()\n",
    "print(\"Gradients of augmented cost w.r.t. y_vec (large init):\", y_vec_large.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_large - lr * y_vec_large.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "6663a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [-0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 1.0]\n",
      "Augmented cost of this solution: 214.0\n",
      "Gradients of cost w.r.t. y_vec (small init): tensor([6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08,\n",
      "        6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08])\n",
      "New y_vec after one gradient step: tensor([0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352,\n",
      "        0.9352], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# what if we make y_vec small?\n",
    "y_vec_small = torch.tensor([p_min for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "loss_small = simulate_one_driver_penalty(y_vec_small, prices, base_demand_all, flex_demand_all)\n",
    "loss_small.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (small init):\", y_vec_small.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_small - lr * y_vec_small.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "1e300279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even if I add a large penalty to the compulsory trade (forced top-ups), the gradient is still pointing down?\n",
    "\n",
    "# when I train using train_pald_contextual.py, this is one of the key issues I run into -- the NN learns that it should predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "d40cb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retry with ridge enabled, but no penalty\n",
    "# define the base demand cvxpylayer\n",
    "pald_base_layer = make_pald_base_layer(K, gamma, ridge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "f3a38b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [0.0, 0.0, 1.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "Cost of this solution: 10.75\n",
      "Gradients of cost w.r.t. y_vec (large init): tensor([6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09,\n",
      "        6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09])\n",
      "New y_vec after one gradient step: tensor([9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936,\n",
      "        9.9936], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_vec_large = torch.tensor([p_max for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "# Run and backprop to inspect gradients w.r.t y_vec\n",
    "loss = simulate_one_driver(y_vec_large, prices, base_demand_all, flex_demand_all)\n",
    "loss.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (large init):\", y_vec_large.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_large - lr * y_vec_large.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "aa03020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [-0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 1.0]\n",
      "Cost of this solution: 14.0\n",
      "Gradients of cost w.r.t. y_vec (small init): tensor([3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10,\n",
      "        3.5394e-10, 3.5394e-10, 3.5394e-10, 3.5394e-10])\n",
      "New y_vec after one gradient step: tensor([0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996,\n",
      "        0.9996], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# what if we make y_vec small?\n",
    "y_vec_small = torch.tensor([p_min for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "loss_small = simulate_one_driver(y_vec_small, prices, base_demand_all, flex_demand_all)\n",
    "loss_small.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (small init):\", y_vec_small.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_small - lr * y_vec_small.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "9781b383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [0.0, 0.0, 1.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "Augmented cost of this solution: 10.75\n",
      "Gradients of augmentedcost w.r.t. y_vec (large init): tensor([6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09,\n",
      "        6.4438e-09, 6.4438e-09, 6.4438e-09, 6.4438e-09])\n",
      "New y_vec after one gradient step: tensor([9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936, 9.9936,\n",
      "        9.9936], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# with penalty and ridge\n",
    "pald_base_layer = make_pald_base_layer(K, gamma, ridge=True)\n",
    "y_vec_large = torch.tensor([p_max for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "# Run and backprop to inspect gradients w.r.t y_vec\n",
    "loss = simulate_one_driver_penalty(y_vec_large, prices, base_demand_all, flex_demand_all)\n",
    "loss.backward()\n",
    "print(\"Gradients of augmentedcost w.r.t. y_vec (large init):\", y_vec_large.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_large - lr * y_vec_large.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "568df746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [-0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 1.0]\n",
      "Augmented cost of this solution: 214.0\n",
      "Gradients of cost w.r.t. y_vec (small init): tensor([6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08,\n",
      "        6.4807e-08, 6.4807e-08, 6.4807e-08, 6.4807e-08])\n",
      "New y_vec after one gradient step: tensor([0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352, 0.9352,\n",
      "        0.9352], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# what if we make y_vec small?\n",
    "y_vec_small = torch.tensor([p_min for i in range(K)], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "loss_small = simulate_one_driver_penalty(y_vec_small, prices, base_demand_all, flex_demand_all)\n",
    "loss_small.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (small init):\", y_vec_small.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 1_000_000\n",
    "new_y_vec = y_vec_small - lr * y_vec_small.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "4f894656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral of piecewise affine function over [0, 1]: -6.50006821629744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/miniconda3/envs/pald/lib/python3.10/site-packages/scs/__init__.py:113: UserWarning: Converting P to a CSC (compressed sparse column) matrix; may take a while.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "def pwq_from_decreasing_affine_samples(x, taus, y):\n",
    "    \"\"\"\n",
    "    Build F(x) = -∫_0^x g(t) dt where g is piecewise affine, decreasing,\n",
    "    specified by its values y at breakpoints taus (taus increasing).\n",
    "    Normalization: F(0)=0 and F'(0)=-g(0).\n",
    "    \"\"\"\n",
    "    taus = np.array(taus, dtype=float)\n",
    "    y = np.array(y, dtype=float)\n",
    "    # segment slopes of g\n",
    "    a = (y[1:] - y[:-1]) / (taus[1:] - taus[:-1])\n",
    "    # curvatures s = -a\n",
    "    s = -a\n",
    "    # hinge-squared weights: w0 = s0, wj = s_j - s_{j-1}\n",
    "    w = np.empty_like(s)\n",
    "    w[0] = s[0]\n",
    "    w[1:] = s[1:] - s[:-1]\n",
    "    # coefficients\n",
    "    c0 = 0.0\n",
    "    c1 = -y[0]  # F'(0) = -g(0)\n",
    "\n",
    "    expr = c0 + c1 * x\n",
    "    for tau, wi in zip(taus[:-1], w):  # no need for the last tau=1\n",
    "        if wi != 0:\n",
    "            expr += 0.5 * wi * cp.square(cp.pos(x - tau))\n",
    "    return expr\n",
    "\n",
    "# Example usage:\n",
    "x1 = cp.Variable()\n",
    "x2 = cp.Variable()\n",
    "taus = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "y    = [10, 9.5, 8.5, 7.0, 5.0, 3.0]\n",
    "\n",
    "F = pwq_from_decreasing_affine_samples(x, taus, y)\n",
    "\n",
    "# compute the integral of the piecewise affine function defined by (taus, y) over [0, 1]\n",
    "prob = cp.Problem(cp.Minimize(F), [x >= 0, x <= 0.8])\n",
    "prob.solve(solver=cp.SCS)\n",
    "print(\"Integral of piecewise affine function over [0, 1]:\", prob.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "39199c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal x: 0.0666666666666668\n",
      "Optimal objective: -0.016666666666667496\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# ---- Define g via samples at breakpoints ----\n",
    "taus = np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])  # breakpoints τ_0..τ_K\n",
    "y     = np.array([10.0, 9.5, 8.5, 7.0, 5.0, 3.0]) # g(τ_j)=y_j (decreasing)\n",
    "\n",
    "p_t = 6.0                                        # price at this time step\n",
    "gamma = 1.0                                      # switching cost parameter for x\n",
    "x_prev = 0.0\n",
    "\n",
    "# segment slopes of g and curvatures of F\n",
    "a = (y[1:] - y[:-1]) / (taus[1:] - taus[:-1])   # slopes of g (≤ 0)\n",
    "s = -a                                          # curvatures of F (≥ 0)\n",
    "w = np.empty_like(s)\n",
    "w[0]  = s[0]\n",
    "w[1:] = s[1:] - s[:-1]                           # hinge-squared weights (≥ 0)\n",
    "\n",
    "c0 = 0.0\n",
    "c1 = -y[0]                                       # F'(τ_0) = -g(τ_0)\n",
    "\n",
    "def F_expr(z):\n",
    "    \"\"\"Return CVXPY expression for F(z) = -∫_{τ0}^z g.\"\"\"\n",
    "    expr = c0 + c1 * z\n",
    "    for tau, wi in zip(taus[:-1], w):            # no need for the last τ_K\n",
    "        if wi != 0:\n",
    "            expr += 0.5 * wi * cp.square(cp.pos(z - tau))\n",
    "    return expr\n",
    "\n",
    "# ---------- Single time step ----------\n",
    "w_const = 0.4                                    # given\n",
    "x = cp.Variable()                                # decision\n",
    "z = w_const + x\n",
    "obj_terms = [p_t * x, gamma * cp.abs(x - x_prev) + gamma * cp.abs(x), F_expr(z) - F_expr(w_const)]        # == -∫_{w}^{w+x} g\n",
    "\n",
    "constraints = [z >= taus[0], z <= taus[-1],      # keep inside domain\n",
    "               x >= 0.0, x <= taus[-1]-w_const]\n",
    "prob = cp.Problem(cp.Minimize(cp.sum(obj_terms)), constraints)\n",
    "prob.solve()\n",
    "\n",
    "print(\"Optimal x:\", x.value)\n",
    "print(\"Optimal objective:\", prob.value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "07ffc9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.00000000e-01 -1.77635684e-14  1.78745907e-14 -1.80411242e-14\n",
      "  3.50000000e+00  0.00000000e+00  1.00000000e+00 -5.32907052e-15\n",
      "  5.32907052e-15  0.00000000e+00]\n",
      "Optimal x: 0.5999975225082667\n",
      "Optimal objective: -0.8799999999846451\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# ---- Define g via samples at breakpoints ----\n",
    "taus = np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])  # breakpoints τ_0..τ_K\n",
    "y     = np.array([10.0, 9.95, 9.9, 9.85, 9.8, 9.4, 9.0, 8.5, 8.0, 7.5, 7.0]) # g(τ_j)=y_j (decreasing)\n",
    "\n",
    "p_t = 5.0                                        # price at this time step\n",
    "gamma = 1.0                                      # switching cost parameter for x\n",
    "x_prev = 0.0\n",
    "\n",
    "# segment slopes of g and curvatures of F\n",
    "a = (y[1:] - y[:-1]) / (taus[1:] - taus[:-1])   # slopes of g (≤ 0)\n",
    "s = -a                                          # curvatures of F (≥ 0)\n",
    "w = np.empty_like(s)\n",
    "w[0]  = s[0]\n",
    "w[1:] = s[1:] - s[:-1]                           # hinge-squared weights (≥ 0)\n",
    "print(w)\n",
    "# truncate w to be non-negative\n",
    "w = np.maximum(w, 0)\n",
    "\n",
    "c0 = 0.0\n",
    "c1 = -y[0]                                       # F'(τ_0) = -g(τ_0)\n",
    "\n",
    "def F_expr(z):\n",
    "    \"\"\"Return CVXPY expression for F(z) = -∫_{τ0}^z g.\"\"\"\n",
    "    expr = c0 + c1 * z\n",
    "    for tau, wi in zip(taus[:-1], w):            # no need for the last τ_K\n",
    "        if wi != 0:\n",
    "            expr += 0.5 * wi * cp.square(cp.pos(z - tau))\n",
    "    return expr\n",
    "\n",
    "# ---------- Single time step ----------\n",
    "w_const = 0.4                                    # given\n",
    "x = cp.Variable()                                # decision\n",
    "z = w_const + x\n",
    "obj_terms = [p_t * x, gamma * cp.abs(x - x_prev) + gamma * cp.abs(x), F_expr(z) - F_expr(w_const)]        # == -∫_{w}^{w+x} g\n",
    "\n",
    "constraints = [z >= taus[0], z <= taus[-1],      # keep inside domain\n",
    "               x >= 0.0, x <= taus[-1]-w_const]\n",
    "prob = cp.Problem(cp.Minimize(cp.sum(obj_terms)), constraints)\n",
    "prob.solve()\n",
    "\n",
    "print(\"Optimal x:\", x.value)\n",
    "print(\"Optimal objective:\", prob.value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "8f25a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try making the layer\n",
    "K=10\n",
    "gamma = 1\n",
    "layer = make_pald_base_layer(K, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "2a15200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from cvxpylayers.torch import CvxpyLayer  # or .jax / .tf\n",
    "\n",
    "# ---- compile-time constants ----\n",
    "taus_full = np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], dtype=float)\n",
    "taus = taus_full[:-1]            # locations of hinge knots (exclude last)\n",
    "tau0, tauK = float(taus_full[0]), float(taus_full[-1])\n",
    "K = len(taus)\n",
    "\n",
    "# ---- decision ----\n",
    "x = cp.Variable()\n",
    "\n",
    "# ---- parameters (layer inputs) ----\n",
    "x_prev  = cp.Parameter(nonneg=True)                 # scalar\n",
    "w_const = cp.Parameter(nonneg=True)                 # scalar\n",
    "p_t     = cp.Parameter(nonneg=True)                 # scalar\n",
    "gamma   = cp.Parameter(nonneg=True)      # scalar\n",
    "\n",
    "# IMPORTANT: pass these precomputed from y (outside the graph)\n",
    "w_hinge = cp.Parameter(K, nonneg=True)   # hinge weights >= 0\n",
    "c1      = cp.Parameter()                 # slope term for F (e.g., -y[0])\n",
    "\n",
    "# ---- auxiliaries (nonlinearities live here, no parameters inside atoms) ----\n",
    "z = w_const + x\n",
    "q = cp.Variable(K)                       # >= (z - tau_j)_+\n",
    "r = cp.Variable(K)                       # >= 0.5 * q_j^2\n",
    "u1 = cp.Variable()                       # >= |x - x_prev|\n",
    "u2 = cp.Variable()                       # >= |x|\n",
    "\n",
    "constraints = [\n",
    "    z >= tau0, z <= tauK,\n",
    "    x >= 0.0, x <= (tauK - w_const),\n",
    "\n",
    "    q >= z - taus,\n",
    "    q >= 0,\n",
    "\n",
    "    cp.square(q) <= 2 * r,   # convex ≤ affine (DPP-safe)\n",
    "    r >= 0,\n",
    "\n",
    "    u1 >=  x - x_prev,  u1 >= -(x - x_prev),\n",
    "    u2 >=  x,           u2 >= -x,\n",
    "]\n",
    "\n",
    "# IMPORTANT: no param*param products in the objective\n",
    "# Use c1 * x (not c1 * z); the dropped c1*w_const is a parameter-only constant → irrelevant to argmin\n",
    "Fz = c1 * x + w_hinge @ r\n",
    "\n",
    "objective = cp.Minimize(p_t * x + gamma * (u1 + u2) + Fz)\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "layer = CvxpyLayer(\n",
    "    prob,\n",
    "    parameters=[x_prev, w_const, p_t, gamma, w_hinge, c1],\n",
    "    variables=[x],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "3689bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def hinge_from_y_torch(taus_full_t: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"\n",
    "    taus_full_t: torch tensor of shape (K+1,), constant breakpoints [τ0,...,τK]\n",
    "    y:           torch tensor of shape (K+1,) or (B, K+1), values g(τ_j)=y_j\n",
    "\n",
    "    Returns:\n",
    "      w_hinge: (K,) or (B, K), nonneg hinge weights\n",
    "      c1:      ()  or (B,),   slope term for F; c1 = -y[..., 0]\n",
    "    \"\"\"\n",
    "    # Ensure taus on same device/dtype as y\n",
    "    taus_full_t = taus_full_t.to(device=y.device, dtype=y.dtype)\n",
    "\n",
    "    # Differences along the last dimension\n",
    "    dt = taus_full_t[1:] - taus_full_t[:-1]                     # (K,)\n",
    "    dy = y[..., 1:] - y[..., :-1]                               # (..., K)\n",
    "\n",
    "    # Slopes of g on segments, then curvatures of F\n",
    "    a = dy / dt                                                 # (..., K)\n",
    "    s = -a                                                      # (..., K)\n",
    "\n",
    "    # Hinge weights are curvature jumps\n",
    "    w0 = s[..., :1]                                             # (..., 1)\n",
    "    wj = s[..., 1:] - s[..., :-1]                               # (..., K-1)\n",
    "    w = torch.cat([w0, wj], dim=-1)                             # (..., K)\n",
    "\n",
    "    # Nonnegativity projection; subgradients pass where w>0\n",
    "    # w = torch.clamp(w, min=0.0)\n",
    "\n",
    "    # c1 = -g(τ0) = -y[..., 0]\n",
    "    c1 = -y[..., 0]\n",
    "\n",
    "    return w, c1\n",
    "\n",
    "\n",
    "# Assume you already constructed `layer = CvxpyLayer(...)` as before.\n",
    "\n",
    "# Constant breakpoints as a torch tensor\n",
    "taus_full_t = torch.linspace(0.0, 1.0, 11, dtype=torch.float32)\n",
    "\n",
    "# Learnable y\n",
    "y_torch = torch.nn.Parameter(torch.tensor(\n",
    "    [10.0, 9.95, 9.9, 9.85, 9.8, 9.4, 9.0, 8.5, 8.0, 7.5, 7.0],\n",
    "    dtype=torch.float32\n",
    "))\n",
    "\n",
    "# Other inputs (scalars)\n",
    "x_prev_t  = torch.tensor(0.0)\n",
    "w_const_t = torch.tensor(0.0)\n",
    "p_t_t     = torch.tensor(7.0)\n",
    "gamma_t   = torch.tensor(1.0)\n",
    "\n",
    "# === forward ===\n",
    "w_hinge_t, c1_t = hinge_from_y_torch(taus_full_t, y_torch)\n",
    "\n",
    "# Call the layer\n",
    "(x_star,) = layer(x_prev_t, w_const_t, p_t_t, gamma_t, w_hinge_t, c1_t)\n",
    "\n",
    "# Example loss using x_star; gradients will flow back to y_torch\n",
    "loss = 0.5 * x_star.pow(2)\n",
    "loss.backward()\n",
    "# y_torch.grad now populated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89a95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -1.,  -2.,  -3.,  -4.,  -5.,  -6.,  -7.,  -8.,  -9., -10.],\n",
      "       grad_fn=<SubBackward0>)\n",
      "tensor([10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
      "        10.0000,  9.9999], grad_fn=<CatBackward0>)\n",
      "tensor(-100., grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Call the layer\n",
    "x_prev_t = torch.tensor(0.0, requires_grad=True)\n",
    "w_const_t = torch.tensor(0.0)\n",
    "p_t_t     = torch.tensor(20.9250)\n",
    "gamma_t   = torch.tensor(10.0)\n",
    "# Learnable y\n",
    "y_torch = torch.nn.Parameter(torch.tensor(\n",
    "    [100,99,97,94,90,85,79,72,64,55,45],\n",
    "    dtype=torch.float32\n",
    "))\n",
    "# print differences between consecutive elements of y_torch\n",
    "print(torch.diff(y_torch))\n",
    "\n",
    "w_hinge_t, c1_t = hinge_from_y_torch(taus_full_t, y_torch)\n",
    "print(w_hinge_t) # ensure that w_hinge is non-negative somehow\n",
    "print(c1_t)\n",
    "(x_star,) = layer(x_prev_t, w_const_t, p_t_t, gamma_t, w_hinge_t, c1_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "230b3c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_star:  tensor(4.7275e-06, grad_fn=<_CvxpyLayerFnFnBackward>)\n"
     ]
    }
   ],
   "source": [
    "# print x_star\n",
    "print(\"x_star: \", x_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d91eb",
   "metadata": {},
   "source": [
    "## retrying the gradient simulation with the new hinge implementation of the cvxpy layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "2c89f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pald_base_layer(K, gamma):\n",
    "    # ---- compile-time constants ----\n",
    "    taus_full = np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], dtype=float)\n",
    "    taus = taus_full[:-1]            # locations of hinge knots (exclude last)\n",
    "    tau0, tauK = float(taus_full[0]), float(taus_full[-1])\n",
    "\n",
    "    # ---- decision ----\n",
    "    x = cp.Variable()\n",
    "\n",
    "    # ---- parameters (layer inputs) ----\n",
    "    x_prev  = cp.Parameter(1,nonneg=True)                 # scalar\n",
    "    w_const = cp.Parameter(1,nonneg=True)                 # scalar\n",
    "    p_t     = cp.Parameter(1,nonneg=True)                 # scalar\n",
    "    gamma   = cp.Parameter(1,nonneg=True)                 # scalar\n",
    "    # IMPORTANT: pass these precomputed from y (outside the graph)\n",
    "    w_hinge = cp.Parameter(K, nonneg=True)   # hinge weights >= 0\n",
    "    c1      = cp.Parameter()                 # slope term for F (e.g., -y[0])\n",
    "\n",
    "    # ---- auxiliaries (nonlinearities live here, no parameters inside atoms) ----\n",
    "    z = w_const + x\n",
    "    q = cp.Variable(K)                       # >= (z - tau_j)_+\n",
    "    r = cp.Variable(K)                       # >= 0.5 * q_j^2\n",
    "    u1 = cp.Variable()                       # >= |x - x_prev|\n",
    "    u2 = cp.Variable()                       # >= |x|\n",
    "\n",
    "    constraints = [\n",
    "        z >= tau0, z <= tauK,\n",
    "        x >= 0.0, x <= (tauK - w_const),\n",
    "\n",
    "        q >= z - taus,\n",
    "        q >= 0,\n",
    "\n",
    "        cp.square(q) <= 2 * r,   # convex ≤ affine (DPP-safe)\n",
    "        r >= 0,\n",
    "\n",
    "        u1 >=  x - x_prev,  u1 >= -(x - x_prev),\n",
    "        u2 >=  x,           u2 >= -x,\n",
    "    ]\n",
    "\n",
    "    # IMPORTANT: no param*param products in the objective\n",
    "    # Use c1 * x (not c1 * z); the dropped c1*w_const is a parameter-only constant → irrelevant to argmin\n",
    "    Fz = c1 * x + w_hinge @ r\n",
    "\n",
    "    objective = cp.Minimize(p_t * x + gamma * (u1 + u2) + Fz)\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "\n",
    "    layer = CvxpyLayer(\n",
    "        prob,\n",
    "        parameters=[x_prev, w_const, p_t, gamma, w_hinge, c1],\n",
    "        variables=[x],\n",
    "    )\n",
    "\n",
    "    return layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "a843194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10           # number of segments in piecewise linear approximation for psi\n",
    "gamma = 1.0     # switching cost parameter for x\n",
    "delta = 0.5     # switching cost parameter for z (used in analytical threshold)\n",
    "S = 1.0          # maximum inventory capacity\n",
    "c_delivery = 0.2\n",
    "eps_delivery = 0.05\n",
    "epochs = 10\n",
    "T = 10\n",
    "\n",
    "# define a toy instance of osdm where prices decrease from 10 to 1 and then jump up to 10, \n",
    "# single unit of base demand at T=10, no flex demand\n",
    "p_min = 1.0\n",
    "p_max = 10.0\n",
    "base_demand_all = [1.0 if t == T-1 else 0.0 for t in range(T)]\n",
    "flex_demand_all = [0.0 for t in range(T)]\n",
    "\n",
    "prices = np.linspace(p_max, p_min, T-1).tolist()\n",
    "prices.append(p_max)\n",
    "\n",
    "solver_options = {\n",
    "    # SCS parameters tend to be robust for differentiable layers\n",
    "    \"eps\": 1e-5,\n",
    "    \"max_iters\": 2000,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def _safe_layer_call(layer, args, size=1.0):\n",
    "    \"\"\"\n",
    "    Call a CvxpyLayer and catch SCS/diffcp failures. Returns x_total tensor.\n",
    "    \"\"\"\n",
    "    (x_total,) = layer(*args, solver_args=solver_options)\n",
    "    return x_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "4323690a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0]\n",
      "Cost of this solution: 62.50201416015625\n",
      "Gradients of cost w.r.t. y_vec (large init): tensor([ 133.5214,  -42.3683, -215.2831,  248.2735, -274.9716,  301.6568,\n",
      "        -301.5762,  301.4978, -300.7552,  226.4803,  -76.4755])\n",
      "New y_vec after one gradient step: tensor([ 9.8665, 10.0424, 10.2153,  9.7517, 10.2749,  9.6983, 10.3015,  9.6984,\n",
      "        10.3007,  9.7734, 10.0764], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# define the base demand cvxpylayer\n",
    "pald_base_layer = make_pald_base_layer(K, gamma)\n",
    "\n",
    "# y_vec is the values of the piecewise affine threshold function -- let's see what happens if we set it HIGH\n",
    "# IMPORTANT: To get valid gradients w.r.t. y_vec, we must keep decisions as torch Tensors and avoid .item()/.round()\n",
    "y_vec_large = torch.tensor([p_max - (i*1e-5) for i in range(K+1)], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "def simulate_one_driver(y_vec, prices, base_demand_all, flex_demand_all, ridge=False):\n",
    "    \"\"\"\n",
    "    Simulate one base driver with given y_vec and return a differentiable loss.\n",
    "    Decisions remain as torch.Tensors so autograd can backprop to y_vec through the CvxpyLayer.\n",
    "    \"\"\"\n",
    "    # torch state (kept differentiable across time)\n",
    "    x_prev = torch.tensor(0.0, dtype=torch.float32).reshape(-1)  # previous decision (scalar)\n",
    "    w_prev = torch.tensor(0.0, dtype=torch.float32).reshape(-1)  # cumulative fraction delivered\n",
    "    gamma_t = torch.tensor(gamma, dtype=torch.float32).reshape(-1)\n",
    "\n",
    "    torch_decisions = []\n",
    "\n",
    "    for t in range(T):\n",
    "        p_t = prices[t]\n",
    "        # Prepare parameters for the layer\n",
    "        p_t_t     = torch.tensor([p_t], dtype=torch.float32)\n",
    "\n",
    "        # Assume you already constructed `layer = CvxpyLayer(...)` as before.\n",
    "\n",
    "        # Constant breakpoints as a torch tensor\n",
    "        taus_full_t = torch.linspace(0.0, 1.0, 11, dtype=torch.float32)\n",
    "\n",
    "        # === forward ===\n",
    "        w_hinge_t, c1_t = hinge_from_y_torch(taus_full_t, y_vec)\n",
    "\n",
    "        # Solve the per-step convex problem via CVXPYLayer (returns x_total, x_parts)\n",
    "        x_total_t = _safe_layer_call(\n",
    "            pald_base_layer, (x_prev, w_prev, p_t_t, gamma_t, w_hinge_t, c1_t)\n",
    "        )\n",
    "\n",
    "        # On the final step, force completion if any remaining fraction < 1.0\n",
    "        if t == T - 1:\n",
    "            # Add just enough slack to finish any remainder (keeps graph intact)\n",
    "            remainder = torch.clamp(1.0 - (w_prev + x_total_t), min=0.0)\n",
    "            x_total_t = x_total_t + remainder\n",
    "\n",
    "        torch_decisions.append(x_total_t.reshape(-1))\n",
    "\n",
    "        # Update state for the next step (kept differentiable)\n",
    "        w_prev = (w_prev + x_total_t).reshape(-1)\n",
    "        x_prev = (x_total_t).reshape(-1)\n",
    "\n",
    "    # Stack decisions and compute differentiable objective\n",
    "    x_seq = torch.stack(torch_decisions)  # shape [T]\n",
    "    z_seq = torch.tensor(base_demand_all, dtype=torch.float32)  # no flex here; constant\n",
    "\n",
    "    delta_t = torch.tensor(delta, dtype=torch.float32)\n",
    "    c_delivery_t = torch.tensor(c_delivery, dtype=torch.float32)\n",
    "    eps_delivery_t = torch.tensor(eps_delivery, dtype=torch.float32)\n",
    "\n",
    "    loss = torch_objective(\n",
    "        torch.tensor(prices, dtype=torch.float32),\n",
    "        x_seq,\n",
    "        z_seq,\n",
    "        gamma_t,\n",
    "        delta_t,\n",
    "        c_delivery_t,\n",
    "        eps_delivery_t,\n",
    "    )\n",
    "\n",
    "    # For visibility only (no graph break for the loss)\n",
    "    with torch.no_grad():\n",
    "        pretty = [round(float(v), 4) for v in x_seq]\n",
    "        print(\"Decisions:\", pretty)\n",
    "        print(\"Cost of this solution:\", float(loss))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "# Run and backprop to inspect gradients w.r.t y_vec\n",
    "loss = simulate_one_driver(y_vec_large, prices, base_demand_all, flex_demand_all)\n",
    "loss.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (large init):\", y_vec_large.grad)\n",
    "# take one step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 0.001\n",
    "new_y_vec = y_vec_large - lr * y_vec_large.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "bf57307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions: [-0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "Cost of this solution: 63.4999885559082\n",
      "Gradients of cost w.r.t. y_vec (small init): tensor([-5.9344e-01,  5.9169e-01,  1.2169e-03,  1.3809e-03, -1.1238e+00,\n",
      "         3.3742e+00, -3.3711e+00,  1.1240e+00,  1.5344e-04, -2.0667e-01,\n",
      "         2.0237e-01])\n",
      "New y_vec after one gradient step: tensor([1.0007, 0.9995, 1.0001, 1.0001, 1.0012, 0.9967, 1.0034, 0.9989, 1.0000,\n",
      "        1.0002, 0.9998], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# what if we make y_vec small?\n",
    "y_vec_small = torch.tensor([p_min + ((K-i)*1e-5) for i in range(K+1)], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "loss_small = simulate_one_driver(y_vec_small, prices, base_demand_all, flex_demand_all)\n",
    "loss_small.backward()\n",
    "print(\"Gradients of cost w.r.t. y_vec (small init):\", y_vec_small.grad)\n",
    "# take one big step in the direction of the gradient to see which direction it's going to take us\n",
    "lr = 0.001\n",
    "new_y_vec = y_vec_small - lr * y_vec_small.grad\n",
    "print(\"New y_vec after one gradient step:\", new_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf14a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pald",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
